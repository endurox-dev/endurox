/**
 * @brief Tuxedo UBB configuration import tool
 *
 * @file ubb2ex.pscript
 */
/* -----------------------------------------------------------------------------
 * Enduro/X Middleware Platform for Distributed Transaction Processing
 * Copyright (C) 2009-2016, ATR Baltic, Ltd. All Rights Reserved.
 * Copyright (C) 2017-2019, Mavimax, Ltd. All Rights Reserved.
 * This software is released under one of the following licenses:
 * AGPL (with Java and Go exceptions) or Mavimax's license for commercial use.
 * See LICENSE file for full text.
 * -----------------------------------------------------------------------------
 * AGPL license:
 *
 * This program is free software; you can redistribute it and/or modify it under
 * the terms of the GNU Affero General Public License, version 3 as published
 * by the Free Software Foundation;
 *
 * This program is distributed in the hope that it will be useful, but WITHOUT ANY
 * WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
 * PARTICULAR PURPOSE. See the GNU Affero General Public License, version 3
 * for more details.
 *
 * You should have received a copy of the GNU Affero General Public License along 
 * with this program; if not, write to the Free Software Foundation, Inc.,
 * 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
 *
 * -----------------------------------------------------------------------------
 * A commercial use license is available from Mavimax, Ltd
 * contact@mavimax.com
 * -----------------------------------------------------------------------------
 */

//
// Ubb config to Enduro/X converter
//
// Some Caveats:
// - In Tuxedo srvid is per group setting. In Enduro/X this setting is per instance.
//   So in case if duplicate srvid's are detected, loader will allocate completely
//   new numbers.
// - Groups are not supported per service entry. Thus to affected machines
//   only first service entry is exported.


////////////////////////////////////////////////////////////////////////////////
// General constants
////////////////////////////////////////////////////////////////////////////////

//Base range for port number.
//Each machine shall be accessible in range of 21001..210NN (where NN max number of
//of cluster nodes).
//Converter from *NETWORK section does not extract ports, just hostnames
const NET_BASE_PORT = 21000;

//Host IP address (assumed) by + nodeid, range 200..2NN (where NN is max cluster node id
//Also assumed that hosts can reach each other by these addresses.
const NET_DEFAULT_IP_START=200;           //start IP of the nodes
const NET_DEFAULT_IP = "192.168.88.";

const IDFREE_SPACE  = 30;                 //Free srvids between new binaries, auto-assign

//Include wizard base.
compilestring(getwizardbase())();

////////////////////////////////////////////////////////////////////////////////
// UBB Config, functions called by ubb2ex as flex & bison parses.
////////////////////////////////////////////////////////////////////////////////

//
// Globals
//

//This will keep open handles
M_instances <- {};

//Mapping from [LMID] -> MACHINE entry
M_lmidmachines <- {};

M_wizzard <- WizardBase();

//
// Parsed UBB Config:
//
M_resources <- {};
M_sections <- {};

//Current values
M_cur_section <- {};
M_cur_default <- {};

//This is hash of arrays.
//For one parameter there may be actually several parameters with the same
//name
M_cur_param <- {};
M_values <- [];


//Folders for generation
//Each key is full path to the disk 
//with following attributes:
//.exists true|false
//.generated true|false
//Used on error termination to cleanup
M_folder_gen <- {};

//List of files generated
//Used on error termination to clean
M_files_gen <- [];

/**
 * Add resource/keyword value
 */
function ubb_add_val(arg)
{
    M_values.append(arg);
}

/**
 * Add resource parameter
 */
function ubb_add_res_parm(arg)
{
    M_resources[arg] <-M_values;
    M_values<-[]; //reset
}

/**
 * Add section parameter
 */
function ubb_add_sect_parm(arg)
{
    local is_default=false;

    print("Adding param ["+arg+"]");

    if (arg == "DEFAULT:")
    {
        is_default=true;
    }

    param <- {};

    //Add link to previous default
    param.defaults <- M_cur_default;
    param.keywords <- {};
    param.name <- arg;
 
    //Refresh current default
    if (is_default)
    {
        param.is_default<-true;
        M_cur_default = param;
    }
    else
    {
        param.is_default<-false;
    }

    //Save current param
    M_cur_param = param;

    //Now this is array
    if (! (arg in M_cur_section.params))
    {
        M_cur_section.params[arg]<-[]
    }

    //Get this individual in the order
    M_cur_section.order.append(param);  
    //This is now array.
    M_cur_section.params[arg].append(param);

}

/**
 * Mark group as participating in routing
 */
function ubb_mark_group_routed(arg)
{
    if (arg in M_sections["*GROUPS"].params 
            && !( "routed" in M_sections["*GROUPS"].params[arg]))
    {
        M_sections["*GROUPS"].params[arg][0].routed<-true;
        print("GROUP ["+arg+"] -> routed");
    }
}

/**
 * Add keyword to parameter, callback
 */
function ubb_add_sect_keyw(arg)
{   
    M_cur_param.keywords[arg] <- M_values;

    if (M_cur_section.name == "*ROUTING" && arg=="RANGES")
    {
        //Parse the DDR range.
        ubb_ddr_parse(M_values[0]);
    }

    M_values<-[];
}

/**
 * Add section, callback
 */
function ubb_add_sect(arg)
{
    M_sections[arg] <- {};
    M_sections[arg].params <- {};
    M_sections[arg].name <- arg;
    M_sections[arg].order <- [];

    M_cur_default <- {};
    M_cur_section = M_sections[arg];
    M_cur_param  <- {};
}

////////////////////////////////////////////////////////////////////////////////
// Generator section
////////////////////////////////////////////////////////////////////////////////


/**
 * Additional initializations
 */
function init()
{
    M_wizzard.qpath = "/dev/mqueue";
    if ("FREEBSD"==getosname())
    {
        M_wizzard.qpath = "/mnt/mqueue";
    }
}

/**
 * Schedule directory creation.
 * Folder creation is performed in recursive way by consulting each section to
 * the schedule and disk existence prior adding to the hash.
 * @param folder folder to create
 */
function schedule_directory(folder)
{
    local comps = split(folder, "/");
    local path="/";
    local len = comps.len();

    for (local i=1; i<len; i++)
    {

        if (path=="/")
        {
            path+=comps[i];
        }
        else
        {
            path+="/"+comps[i];
        }

        if (!(path in M_folder_gen) && !fileexists(path))
        {            
            M_folder_gen[path]<-{};
        }
    }
}

/**
 * Get default value for section param.
 * Note that platformscript does not allow to use identifier "default" as
 * variable name.
 * @param param param object of the section
 * @param keyword keyword of interest
 * @param stock_default stock default value (if param value not present, and any
 * chained default value is not found
 * @return action value read
 */
function get_val(param, keyword, stock_default)
{
    if (! (keyword in param.keywords))
    {
        //Lookup in reverse order from current default
        local dflt = param.defaults;

        while (("keywords" in dflt) && !(keyword in dflt.keywords))
        {
            //Step back to previous default
            if ("defaults" in dflt)
            {
                dflt = dflt.defaults;
            }
            else
            {
                //No more defaults in the section
                break;
            }
        }

        if ("keywords" in dflt && keyword in dflt.keywords)
        {
            return dflt.keywords[keyword];
        }
        else
        {
            if (stock_default==null)
            {
                throw("No stock "+keyword+" value available for param ["+param.name+"]");
            }
            return stock_default;
        }
    }
    else
    {
        return param.keywords[keyword];
    }
}

/**
 * Return number of services for instance used for routing
 * @param instance instance of interest
 * @return max number of routing services used
 */
function get_routing_services(instance)
{
    local cnt=0;

    if ("*SERVICES" in M_sections)
    {
        foreach(idx,service in M_sections["*SERVICES"].order)
        {
            if (!service.is_default)
            {
                local range = {};
                local service_srvgrp = get_val(service, "SRVGRP", "");
                
                //Service may have no group -> affect all
                if (service_srvgrp =="")
                {
                    cnt++;
                }
                else
                {
                    local group_lmid = get_val(
                        M_sections["*GROUPS"].params[service_srvgrp[0]][0], "LMID", null)[0];

                    if (group_lmid == instance.lmid)
                    {
                        cnt++;
                    }

                }
            }
        }
    }
    print("Routing services: "+cnt);
    return cnt;
}

/**
 * Reserve number of slots in free ranges
 * remove those ranges from next time use.
 * @param instance current machine instance we are generating
 * @slots number of slots required
 * @return start index that is guaranteed to have + (slots-1) free
 *  places.
 */
function reserve_range(instance, slots)
{
    local ret = 0;

    foreach(idx,range in instance.free_ranges)
    {
        if (range.min + (slots-1) <= range.max)
        {
            ret=range.min;
            //Reduce
            range.min=range.min+slots;
            
            if (range.min>range.max)
            {
                instance.free_ranges.remove(idx);
            }
            break;
        }
    }

    if (0==ret)
    {
        throw("Cannot find free binary range for number of "+slots+" slots");
    }

    return ret;
}

/**
 * Prepare free ID ranges for given instance. For upper number we take
 *  number of total servers + 2000.
 * @param instance instance of interest
 */
function prep_free_ranges(instance)
{
    //We need to register all ranges
    //All ranges must be sorted
    //The free interval must be detected
    local start = 1;
    instance.ranges<-[];

    if ("*SERVERS" in M_sections) foreach(idx,server in M_sections["*SERVERS"].order)
    {
        if (!server.is_default)
        {
            local range = {};
            local server_srvgrp = get_val(server, "SRVGRP", "");
            local group_lmid = get_val(
                M_sections["*GROUPS"].params[server_srvgrp[0]][0], "LMID", null)[0];

            if (group_lmid == instance.lmid)
            {
                //OK this is our server
                range.min <-get_val(server, "MIN", ["1"]);
                range.max <-get_val(server, "MAX", range.min);
                range.srvid <-get_val(server, "SRVID", "");

                if (range.max[0].tointeger() < range.min[0].tointeger())
                {
                    throw(format("Invalid server %s range min=%d > max=%d",
                            server.name, range.min[0].tointeger()
                            , range.max[0].tointeger()));
                }
                range.min = range.min[0].tointeger()+range.srvid[0].tointeger();
                range.max = range.max[0].tointeger()+range.srvid[0].tointeger();
                    
                //print(format("Used rang srvid: %s %d - %d", 
                //    range.srvid[0], range.min[0], range.max[0]));
                instance.ranges.append(range);
            }
        }    
    }

    instance.ranges.sort(@(a,b) a.min <=> b.min);
    instance.free_ranges <- [];

    foreach(idx,range in instance.ranges)
    {
        print(format("Used range %d - %d", 
                range.min, range.max));
    }

    //Add first range if any
    local len = instance.ranges.len();
    if (len > 0)
    {
        if (instance.ranges[0].min > 1)
        {
            local fr = {};
            fr.min <- 1;
            fr.max <- (instance.ranges[0].min - 1);
            instance.free_ranges.append(fr);
        }

        local i=0;
        for (; i<len-1; i++)
        {
            if (instance.ranges[i].max+1< instance.ranges[i+1].min)
            {
                local fr = {};
                fr.min <- (instance.ranges[i].max   + 1);
                fr.max <- (instance.ranges[i+1].min - 1);
                instance.free_ranges.append(fr);
            }
        }

        //Add some free range+10K
        local fr = {};
        fr.min <- (instance.ranges[i].max   + 1);
        fr.max <- (instance.ranges[i].max   + 1 + 10000);
        instance.free_ranges.append(fr);
    }
    else
    {
        local fr = {};
        fr.min <- 1;
        fr.max <- 10000;
        instance.free_ranges.append(fr);
    }

    foreach(idx,range in instance.free_ranges)
    {
        print(format("Free range %d - %d", 
                range.min, range.max));
    }

}

/**
 * Prepare groups of interest for this particular node.
 * Mark the group is it used or not (used if routed or have xa).
 * Prepare open/close infos / xa infos, (extract xa settings).
 */
function prep_groups(instance)
{
    foreach(idx,igroup in M_sections["*GROUPS"].params)
    {
        //Single group only supported...
        local group = igroup[0];
        //Get our groups
        if ( !group.is_default && (get_val(group, "LMID", null)[0] == instance.lmid))
        {
            group.plot<-false;
            group.tmsrv_plotted<-false;

            local ubb_openinfo = get_val(group, "OPENINFO", [""])[0];
            local ubb_tmsname = get_val(group, "TMSNAME", [""])[0];

            if (ubb_tmsname!="")
            {
                //Mark that this instance uses xa.
                instance.xa_used<-true;

                group.rmid <- get_val(group, "GRPNO", null)[0];
                local openinfo = ubb_openinfo;

                //Get the Switch name
                local ex = regexp(@"^(.*):.*");

                local res = {};
                if (openinfo!="")
                {
                    local cap = ex.capture(openinfo);
                    if (null!=cap)
                    {
                        local res = cap[1];
                        group.xaswitchname <-openinfo.slice(res.begin, res.end);
                    }
                    else
                    {
                        group.xaswitchname <-"";
                    }
                }
                else
                {
                    group.xaswitchname <-"";
                }
                //Save TMS too..
                group.tmsname<-ubb_tmsname;

                print("Got switch: ["+group.xaswitchname+"]");
                
                //Support for null switch
                if (group.tmsname=="TMS")
                {
                     group.openinfo <- "-";
                     group.closeinfo <- "-";
                     group.driverlib<-"libndrxxanulls."+M_wizzard.shared_lib_pfx;
                     group.rmlib<-"-";
                }
                else if (group.xaswitchname== "TUXEDO/QM")
                {
                    //Special case for MQ
                    //We only need Qspace name, the data will be stored
                    // in app_home/qdata/rm<rmid>

                    ex = regexp(@"^.*:.*:(.*)");
                    res = ex.capture(openinfo)[1];
                    local qspace = openinfo.slice(res.begin,res.end);
                    group.openinfo <- "datadir=\"${NDRX_APPHOME}/qdata/"+qspace+"\",qspace=\""+qspace+"\"";
                    group.closeinfo <- "${NDRX_XA_OPEN_STR}";
                    
                    //Schedule folder for creation.
                    if ( !(instance.app_home+"/qdata" in M_folder_gen))
                    {
                        //M_folder_gen[instance.app_home+"/qdata"]<-{};
                        schedule_directory(instance.app_home+"/qdata");
                    }
                    group.qspace<-qspace;
                    group.data_folder <- instance.app_home+"/qdata/"+qspace;
                    //M_folder_gen[group.data_folder]<-{};
                    schedule_directory(group.data_folder);

                    //Hashmap of auto-queues served by given queue space
                    //internally may contains .trantime setting override.
                    group.auto_queues<-{};
                    group.workers<-0;

                }
                else
                {
                    ex = regexp(@"^.*:(.*)");
                    res = ex.capture(openinfo)[1];
                    //Extract the values, transform the "TUXEDO/QM"
                    group.openinfo <- openinfo.slice(res.begin,res.end);
                    local closeinfo = get_val(group, "CLOSEINFO", [""])[0];
                    
                    if (closeinfo=="")
                    {
                        group.closeinfo <- "${NDRX_XA_OPEN_STR}";
                    }
                    else
                    {
                        ex = regexp(@"^.*:(.*)");
                        res = ex.capture(closeinfo)[1];
                        group.closeinfo <- closeinfo.slice(res.begin,res.end);
                    }
                }

                //Schedule folders to be created
                if ( !(instance.app_home+"/tmlogs" in M_folder_gen))
                {
                    schedule_directory(instance.app_home+"/tmlogs");
                }

                group.tmlogs<-instance.app_home+"/tmlogs/rm"+group.rmid;
                //M_folder_gen[group.tmlogs]<-{};
                schedule_directory(group.tmlogs);

            }

        }
    }
}

/**
 * Merge defaults for current instance
 */
function merge_defaults(instance)
{
    local prev_server={};
    prev_server.is_default<-false;
    foreach (idx, server in instance.servers)
    {
        //If current one is default and previous is also de
        if (server.is_default && prev_server.is_default)
        {
            //Copy all current server attribs over prev_server
            //and delete curren entry.
            foreach (idx, keyword in server)
            {
                prev_server[idx]<-server[idx];
            }
            //Remove current entry...
            instance.servers[idx].deleted<-true;
        }
        else
        {
            //Ensure deleted tags
            if (!("deleted" in instance.servers[idx]))
            {
                instance.servers[idx].deleted<-false;
            }
            prev_server = server;
        }
        
        //Mark group used...
        if ("SRVGRP" in server)
        {
            M_sections["*GROUPS"].params[server["SRVGRP"]][0].plot<-true;
        }
    }
    
}

/**
 * Escape string, includes escape for double quotes, and xml escape of 
 * <>&
 * @param str string to escape
 * @param attr is this XML attribute value?
 * @return escaped string
 */
function escape_clopt_xml(str, attr)
{
    local ret = "";
    foreach (i, c in str)
    {
        local chr=format("%c", c);

        switch (chr)
        {
            case "\"":
                if (attr)
                {
                    chr = "&quot;";
                }
                else
                {
                    chr = "\\"+chr;
                }
                break;
            case "&":
                chr = "&amp;";
                break;
            case "<":
                chr = "&lt;";
                break;
            case ">":
                chr = "&gt;";
                break;
        }

        ret=ret+chr;
    }

    return ret;
}

/**
 * Prepare binaries:
 * For this LMID:
 * Remove: WSL/TMSYSEVT/JSL/TMMETADATA/TMFAN/TMQFORWARD
 * Mark: if found TMSYSEVT or TMUSREVT -> tpevsrv needed.
 * Mark: If WSL or JSL used -> restincl needed (incl clients section) & cpmsrv
 * Extract: Group qspace shall be appended with automatic Qs from TMQFORWARD instances.
 * Transform: If for server -A is found, remove it. If -A is not found, add -N
 *  as Enduro/X advertises all by default.
 * Transform: TMQUEUE -> tmqueue, replace min/max=1, update clopt.
 */
function prep_servers(instance)
{
    local server_optstring = "Aa:s:e:Ghl:n:o:Pp:rtv";

    // phase 1. Get infos about the system
    if ("*SERVERS" in M_sections) foreach(idx,server in M_sections["*SERVERS"].order) if (!server.is_default)
    {
        local server_srvgrp = get_val(server, "SRVGRP", null);
        local group_lmid =get_val(M_sections["*GROUPS"].params[ server_srvgrp[0] ][0], "LMID", null)[0];

        if (group_lmid == instance.lmid)
        {
            local remove = false;
            print("Processing binary: ["+server.name+"]");
            
            /* Detect the type of the binary */

            switch(server.name)
            {
                case "WSL":
                case "JSL":
                        instance.restin<-true;
                        remove=true;
                    break;
                case "TMSYSEVT":
                case "TMUSREVT":
                        instance.events<-true;
                        remove=true;
                    break;
                case "TMMETADATA":
                case "TMFAN":
                        remove=true;
                    break;
                case "TMQUEUE":
                    //Translate to tmqueue
                    server.name="tmqueue";
                    server.keywords["MIN"]<-["1"];
                    server.keywords["MAX"]<-["1"];
                    //Match the forwarders at plotting, by group lookup...
                    server.keywords["CLOPT"]<-["-A -e ${NDRX_ULOG}/tmqueue.${NDRX_SVSRVID}.log -r -- -s1 -p10"];
                    
                    break;
                case "TMQFORWARD":

                        remove=true;
                        
                        local group = M_sections["*GROUPS"].params[server_srvgrp[0]][0];
                        //Parse the group of the forward
                        //and parse the clopt of forward, so that we get
                        //queue name & 
                        //Extract min setting (used to set workers for the Q)
                        //Extract -t from clopt second group
                        //Extract -q Q1,Q2,etc. from clopt second group
                        
                        local fwd_min = get_val(server, "MIN", ["1"])[0];
                        local fwd_clopt = get_val(server, "CLOPT", ["-A"])[0];
                            
                        //server opt string + tmqforward optstring
                        local clopt_parsed = parseclopt2(fwd_clopt, 
                                server_optstring, "q:t:ib:ednf:");

                        //assume -1 no special timeout used.
                        local trantime = "-1";

                        foreach(idx,opt in clopt_parsed.args2)
                        {
                            if (opt.opt=="t")
                            {
                                trantime= opt.val;
                                break;
                            }
                        }

                        foreach(idx,opt in clopt_parsed.args2)
                        {
                            if (opt.opt=="q")
                            {
                                //Split Q by ,
                                //And load each Q
                                local qs = split(opt.val, ",");
                                foreach (idx, qq in qs)
                                {  
                                    //Must be loaded.
                                    print(format("Adding Q [%s] to group [%s] trantime: %s",
                                                qq, group.name, trantime));
                                    group.auto_queues[qq]<-{};
                                    group.auto_queues[qq].queue<-qq;
                                    group.auto_queues[qq].trantime<-trantime;
                                    group.auto_queues[qq].workers<-fwd_min;
                                    group.workers+=fwd_min.tointeger();
                                }
                            }
                        }
                    break;
            }

            //do not process the deleted servers
            if (remove)
            {
                server.deleted<-true;
            }
            
        }   
    }


    // phase 2. Prepare binaries into instance.servers array, each element is hash with
    // key settings for the server or default to be generated.
    instance.servers<-[]

    local new_srv = {};
    
    //Add some reasonable defaults (mainly required by Enduro/X
    //and may be merged later if we get some further defaults
    new_srv.min<-1;
    new_srv.max<-1;
    new_srv.autokill<-1;
    new_srv.start_max<-10;
    new_srv.pingtime<-100;
    new_srv.ping_max<-800;
    new_srv.end_max<-10;
    new_srv.killtime<-1;
    new_srv.respawn<-"Y";
    new_srv.is_default<-true;
    instance.servers.append(new_srv);

    //Add common configuration server
    new_srv = {};
    new_srv.bin<-"cconfsrv";
    new_srv.srvid <- reserve_range(instance, 2);
    new_srv.sysopt <- "-e ${NDRX_ULOG}/cconfsrv.${NDRX_SVSRVID}.log -r";
    new_srv.min<-2;
    new_srv.max<-2;
    new_srv.is_default<-false;
    instance.servers.append(new_srv);

    //Add tpadmin server
    new_srv = {};
    new_srv.bin<-"tpadmsv";
    new_srv.srvid <- reserve_range(instance, 2);
    new_srv.sysopt <- "-e ${NDRX_ULOG}/tpadmsv.${NDRX_SVSRVID}.log -r";
    new_srv.min<-2;
    new_srv.max<-2;
    new_srv.is_default<-false;
    instance.servers.append(new_srv);

    //Add event server if used.
    if ("events" in instance)
    {
        new_srv = {};
        new_srv.bin<-"tpevsrv";
        new_srv.srvid <- reserve_range(instance, 1);
        new_srv.sysopt <- "-e ${NDRX_ULOG}/tpevsrv.${NDRX_SVSRVID}.log -r";
        new_srv.min<-1;
        new_srv.max<-1;
        new_srv.is_default<-false;
        instance.servers.append(new_srv);
    }

    //Add networking if used.
    if ( instance.machine.networked)
    {
        //link this machine with other networked machines.
        //the order of the machines
        //lets without -f, add manually if different architecture hosts
        //have been found.
        foreach(idx,val in M_sections["*MACHINES"].params) 
        {
            local machine = val[0];

            if (machine.is_default || !machine.networked)
            {
                continue;
            }

            local lmid  = machine.keywords.LMID[0];

            new_srv = {};
            new_srv.bin<-"tpbridge";
            new_srv.srvid <-reserve_range(instance, 1);
            new_srv.min<-1;
            new_srv.max<-1;
            new_srv.is_default<-false;

            if (machine.nodeid < instance.nodeid)
            {
                //In this case we take passive role and accept incoming connections
                new_srv.sysopt<-("-e ${NDRX_ULOG}/tpbridge."+instance.nodeid+ "."+machine.nodeid+".log");

                new_srv.appopt<-"-n"+machine.nodeid+" -r -i 0.0.0.0 -p "
                        +(NET_BASE_PORT+machine.nodeid)+" -tP -z30";

                instance.servers.append(new_srv);
            }
            else if (machine.nodeid > instance.nodeid)
            {

                //This is active role, we connect 
                new_srv.sysopt<-"-e ${NDRX_ULOG}/tpbridge." 
                        +instance.nodeid+ "."+machine.nodeid+".log";

                if ("ip" in machine)
                {
                    new_srv.appopt<-"-n"+machine.nodeid+" -r -i "+machine.ip+" -p "
                        +(NET_BASE_PORT+instance.nodeid)+" -tA -z30";
                }
                else
                {
                    new_srv.appopt<-"-n"+machine.nodeid+" -r -h "+machine.hostname+" -p "
                        +(NET_BASE_PORT+instance.nodeid)+" -tA -z30";
                }

                instance.servers.append(new_srv);
            }
        }
    }
    
    //loop over the binaries & defaults, if not deleted, add (for our instance)
    if ("*SERVERS" in M_sections) foreach(idx,server in M_sections["*SERVERS"].order) if (! ("deleted" in server))
    {
        local proceed=false;
        local server_srvgrp = "";
        local group = {};
        if (server.is_default)
        {
            //Any default is OK
            proceed=true;
        }
        else
        {
            server_srvgrp = get_val(server, "SRVGRP", null)[0];
            group = M_sections["*GROUPS"].params[server_srvgrp][0];
            local group_lmid =get_val(group, "LMID", null)[0];
            if (group_lmid==instance.lmid)
            {
                proceed=true;
            }
        }
        
        if (proceed)
        {
            //Process the binary, add to arrays...
            new_srv = {};
            
            if (server_srvgrp!="")
            {
                new_srv.srvgrp <-server_srvgrp;
                new_srv.group<-group;
            }

            new_srv.bin <- server.name;
            new_srv.is_default <- server.is_default;
            
            if (!new_srv.is_default)
            {
                //Load clopt & app opt
                local clopt = get_val(server, "CLOPT", ["-A"])[0];
                local clopt_parsed = parseclopt1(clopt, server_optstring);
                local have_A=false;
                local have_e=false;
                local sysopt = "";
                local appopt = "";

                //Check is -A not present, if so then we need to set -N
                foreach(idx,opt in clopt_parsed.args1)
                {
                    if (opt.opt=="A")
                    {
                        have_A=true;
                    }
                    
                    if (opt.opt=="e")
                    {
                        have_e=true;
                    }
                }
                
                if (!have_A)
                {
                    sysopt = "-N";
                }

                //Add -e file to write the stderr to log files.
                if (!have_e)
                {
                    if (sysopt!="")
                    {
                        sysopt= sysopt + " " + "-e ${NDRX_ULOG}/"+new_srv.bin+".${NDRX_SVSRVID}.log"; 
                    }
                    else
                    {
                        sysopt= "-e ${NDRX_ULOG}/"+new_srv.bin+".${NDRX_SVSRVID}.log"; 
                    }
                }

                //If string contains tab, space or newline - add quotes
                //For both sysopt and appopt.

                //Now generate clopt...
                foreach(idx,opt in clopt_parsed.args1) if (opt.opt != "A")
                {
                    if (sysopt=="")
                    {
                        sysopt="-"+opt.opt;
                    }
                    else
                    {
                        sysopt=sysopt + " -"+opt.opt;
                    }
                    if ("val" in opt)
                    {
                        //Escape " if found
                        if (null!=opt.val.find(" ") || null!=opt.val.find("\n") || null!=opt.val.find("\t"))
                        {
                            sysopt=sysopt + " \"" + escape_clopt_xml(opt.val, false) + "\"";
                        }
                        else
                        {
                            sysopt=sysopt + " " + escape_clopt_xml(opt.val, false);
                        }
                    }
                }

                //Generate app opts
                foreach(idx,opt in clopt_parsed.freeargs)
                {
                    if (appopt=="")
                    {
                        appopt=opt;
                    }
                    else
                    {
                        if (null!=opt.find(" ") || null!=opt.find("\n") || null!=opt.find("\t"))
                        {
                            appopt=appopt + " \"" + escape_clopt_xml(opt, false) + "\"";
                        }
                        else
                        {
                            appopt=appopt + " " + escape_clopt_xml(opt, false);
                        }
                    }
                }
                
                print("Built sysopt ["+sysopt+"]");
                print("Built appopt ["+appopt+"]");
                new_srv.sysopt <- sysopt;
                new_srv.appopt <- appopt;
            }

            if ("SRVGRP" in server.keywords)
            {
                new_srv.cctag<-server.keywords.SRVGRP[0];
            }

            if ("SRVID" in server.keywords)
            {
                new_srv.srvid<-server.keywords.SRVID[0];
            }

            if ("MIN" in server.keywords)
            {
                new_srv.min<-server.keywords.MIN[0];
            }

            if ("MAX" in server.keywords)
            {
                new_srv.max<-server.keywords.MAX[0];
            }
            else if ("min" in new_srv)
            {
                new_srv.max<-new_srv.min;
            }
                
            if (!server.is_default)
            {
                //resolve min/max for final check
                new_srv.real_min <- get_val(server, "MIN", ["1"])[0].tointeger();
                //Default MAX is same as MIN
                new_srv.real_max <-  get_val(server, "MAX", [new_srv.real_min])[0].tointeger();
            }

            if ("ENVFILE" in server.keywords)
            {
                new_srv.env<-server.keywords.ENVFILE[0];
            }

            //Not used on Linux
            if ("RQADDR" in server.keywords)
            {
                new_srv.rqaddr<-server.keywords.RQADDR[0];
            }
            
            if ("RESTART" in server.keywords)
            {
                new_srv.respawn<-server.keywords.RESTART[0];
            }


            //If MAXDISPATCHTHREADS<=1, then no need for this setting at all
            //Except, we shall understand do we get something from previous defaults
            //If there is no previous default, then do not plot this
            //If there is previous default and it differs form this setting, then we need to plot
            //the thing, also if max=1, then min shall be set to 1 too...

            //Get default
            local default_max=1;
            local default_min=1;

            if ("keywords" in server.defaults)
            {
                default_max = get_val(server.defaults, "MAXDISPATCHTHREADS", ["1"])[0].tointeger();
                default_min = get_val(server.defaults, "MINDISPATCHTHREADS", ["1"])[0].tointeger();
            }

            if ("MAXDISPATCHTHREADS" in server.keywords)
            {
                local thrds = server.keywords.MAXDISPATCHTHREADS[0].tointeger();
                //Set only if default was bigger than 1
                if (thrds<=1 && default_max>1 || thrds>1)
                {
                    //Enduro/X default is 1
                    if (thrds<1)
                    {
                        thrds=1;
                    }
                    new_srv.maxdispatchthreads<-thrds;
                }
            }

            if ("MINDISPATCHTHREADS" in server.keywords)
            {
                local thrds = server.keywords.MINDISPATCHTHREADS[0].tointeger();
                //Set only if default was bigger than 1
                if (thrds<=1 && default_min>1 || thrds>1)
                {
                    //Enduro/X default is 1
                    if (thrds<1)
                    {
                        thrds=1;
                    }
                    new_srv.mindispatchthreads<-thrds;
                }
            }

            //THREADSTACKSIZE this goes to process, thus read value from defaults...
            local stacksz = get_val(server, "THREADSTACKSIZE", ""); 

            if (stacksz!="")
            {
                //Our stack is in KB
                new_srv.threadstacksize<- (stacksz[0].tointeger() / 1024);
            }
            
            instance.servers.append(new_srv);
        }

    }

    if (instance.xa_used || instance.restin)
    {
        //Add new defaults, do not re-use cctag
        new_srv={};
        new_srv.min<-1;
        new_srv.max<-1;
        new_srv.respawn<-"Y";
        new_srv.is_default<-true;
        new_srv.cctag<-"/"; //Use root global settings
        instance.servers.append(new_srv);

        if (instance.xa_used)
        {
            //Add new defaults, do not re-use cctag
            new_srv={};
            new_srv.bin <- "tmrecoversv";
            new_srv.srvid <- reserve_range(instance, 1);
            new_srv.real_min<-1;
            new_srv.real_max<-1;
            new_srv.is_default<-false;
            new_srv.sysopt <- "-e ${NDRX_ULOG}/tmrecoversv.log -r";
            new_srv.appopt <- "-p -s10";
            new_srv.cctag<-"/"; //Allow merge
            instance.servers.append(new_srv);
        }

        if (instance.restin)
        {
            //Add new defaults, do not re-use cctag
            new_srv={};
            new_srv.bin <- "cpmsrv";
            new_srv.srvid <- reserve_range(instance, 1);
            new_srv.real_min<-1;
            new_srv.real_max<-1;
            new_srv.is_default<-false;
            new_srv.sysopt <- "-e ${NDRX_ULOG}/cpmsrv.log -r";
            new_srv.appopt <- "-k3 -i1";
            new_srv.cctag<-"/"; //Allow merge
            instance.servers.append(new_srv);
        }
    }


    //phase 4. Merge consecutive defaults, remove last default if no binaries follow
    merge_defaults(instance);

    //OK Ready to plot debug, forward queues, server defaults + servers.
    local servers_final=[];

    // Copy all servers to final list
    // in case if xa is used, add tmsrv firstly
    foreach (idx, server in instance.servers)
    {
        //Check does this depend on XA group?
        //srvgrp is for translated binaries. which basically pull in the tmsrv
        if (!server.is_default && "srvgrp" in server &&
                !server.deleted)
        {
            local group = M_sections["*GROUPS"].params[ server.srvgrp ][0];
            local tmscout =get_val(group, "TMSCOUNT", [""])[0];
            
            //This is tmsrv group
            if ("tmsname" in group && !group.tmsrv_plotted)
            {
                local cnt = 1;
                if (tmscout!="")
                {
                    cnt = tmscout.tointeger();
                }
                //Add tmsrv here
                //Keep the same group setting
                //either direct cctag (if set) or then it must come from
                //the default.
                new_srv = {};

                
                if (group.xaswitchname== "TUXEDO/QM" || group.tmsname=="TMS")
                {
                    new_srv.bin<-"tmsrv";
                }
                else
                {
                    new_srv.bin<-get_val(group, "TMSNAME", null)[0];
                }

                new_srv.srvid <- reserve_range(instance, cnt);
                new_srv.sysopt <- "-e ${NDRX_ULOG}/tmsrv.${NDRX_SVSRVID}.log -r";
                new_srv.appopt <- "-t1 -l"+group.tmlogs;
                new_srv.min<-cnt;
                new_srv.max<-cnt;
                if ("cctag" in server)
                {
                    new_srv.cctag<- server.cctag;
                }
                new_srv.is_default<-false;
                group.tmsrv_plotted=true;
                servers_final.append(new_srv);
            }

            //perform correction on tmq so that forward thread pool
            if (server.bin=="tmqueue")
            {
                //get the group & forwarder count
                //workers
                local workers = server.group.workers;

                if (workers<10)
                {
                    workers=10;
                }

                server.appopt+=" -f"+workers;
            }
        }

        if (!server.deleted)
        {
            servers_final.append(server);
        }
    }

    instance.servers = servers_final;

    //Remove any un-needed defaults from the end
    local len = instance.servers.len();
    
    for (local i=len-1; i>=0; i--)
    {
        local server = instance.servers[i];
        if (server.is_default)
        {
            instance.servers.remove(i);
        }
        else
        {
            break;
        }
    }

    //mark groups to plot
    foreach (idx, server in instance.servers)
    {
        if ("group" in server)
        {
            server.group.plot=true;
        }
    }

    //Merge from bellow to to up.
    //if all servers bellow default have the same group and default have also
    //group set, then set common group in the default and clear the cctag
    //setting from bellow servers
    local len = instance.servers.len();
    local last_cctag = "";
    local first = false;
    local continue_till_default = false;
    local matching_servers = [];

    for (local i=len-1; i>=0; i--)
    {
        local server = instance.servers[i];

        if (i==len-1)
        {
            first=true;
        }

        if (server.is_default)
        {
            if (!continue_till_default && last_cctag!="")
            {
                //Merge the cctag to default
                server.cctag<-last_cctag;
                //Remove default from all linked servers
                
                foreach (idx, sv in matching_servers)
                {
                    delete sv.cctag;
                }

            }
            else
            {
                //If and there is no default cctag for this default
                //then stop merging.
                //Because then further default merges might affect this particular
                //group and following binaries (i.e. get cctag from previous default)
                //Thus stop merging here.
                break;
            }

            first = true;
            last_cctag="";
            matching_servers = [];
        }
        else if (continue_till_default)
        {
            continue;
        }
        else if ("cctag" in server)
        {
            if (first)
            {
                last_cctag = server.cctag;
                //Append in case if we want to merge the group...
                matching_servers.append(server);
                first=false;
            }
            else if (last_cctag != server.cctag)
            {
                last_cctag="";
                continue_till_default=true;
            }
            else
            {
                matching_servers.append(server);
            }
        }
        else
        {
            continue_till_default=true;
        }
    }

    //Verify server IDs
    local id_check = {};
    local have_dup = false;

    foreach (idx, server in instance.servers)
    {
        if (server.is_default)
        {
            continue;
        }

        local min = 0;
        local max = 0;

        if ("min" in server)
        {
            min = server.min;
            //For further processing
            server.real_min<-min;
        }
        else
        {
            min = server.real_min;
        }

        if ("max" in server)
        {
            max = server.max;
            //For further processing
            server.real_max<-max;
        }
        else
        {
            max = server.real_max;
        }

        print("BIN "+server.bin+" srvid="+server.srvid+" max="+max);

        for (local i=server.srvid.tointeger(); i<server.srvid.tointeger()+max.tointeger(); i++)
        {
            print("Check: "+i);
            if (i in id_check)
            {
                error("Got duplicate id ["+i+"] -> fallback to new assign");
                have_dup = true;
                //break; keep looping needs to setup all real_max
            }
            
            id_check[i]<-true;
        }

    }

    //IDFREE_SPACE
    //Loop over the all binaries 
    local srvid_base=1;
    if (have_dup || "1"== M_opt_A)
    {
        print("Assigning new numbers...");
        foreach (idx, server in instance.servers)
        {
            if (server.is_default)
            {
                continue;
            }
            
            server.srvid=srvid_base;
            srvid_base+=(server.real_max.tointeger()+IDFREE_SPACE);
        }
    }

}

/**
 * prepare networking. This will update each
 * As we need range of ports. we will extract only ip address from.
 * IPv6 recognition during migration currently is not supported (just add manually
 * to the tpbridge)
 * machine with LMID with ip/host/binding port information
 */
function prep_networking()
{   
    if (!("*NETWORK" in M_sections))
    {
        return;
    }
     
    // Assign each ip addr/or hostname, and add networked param
    foreach(idx,net in M_sections["*NETWORK"].params) if (!net[0].is_default)
    {
        local net = net[0];
        local naddr = get_val(net, "NADDR", null)[0];
        print(format("parsing lmid=[%s] naddr=[%s]", net.name, naddr));
        M_lmidmachines[idx].networked<-true;

        if (regexp("^//[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}:.*").match(naddr))
        {
            local ex = regexp("^//([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3}):.*");
            local res = ex.capture(naddr);
            local ip = naddr.slice(res[1].begin,res[1].end)+"."+
                        naddr.slice(res[2].begin,res[2].end)+"."+
                        naddr.slice(res[3].begin,res[3].end)+"."+
                        naddr.slice(res[4].begin,res[4].end);
            print("Extract by ip: "+ip);
            M_lmidmachines[idx].ip <- ip;
            
        }
        else if (regexp("^//.*:.*").match(naddr))
        {
            local ex = regexp(@"^//(.*):.*");
            local res = ex.capture(naddr);
            local hostname = naddr.slice(res[1].begin,res[1].end);
            print("Extract by hostname: "+hostname);
            M_lmidmachines[idx].hostname <- hostname;
        }
        /* extract by hex */
        else if (regexp("^//[xX]0002[0-9A-Fa-f]{12}").match(naddr)
                || regexp("^0[xX]0002[0-9A-Fa-f]{12}").match(naddr))
        {
            local hex_str = "";
            
            if ("//"==substr(naddr, 0, 2))
            {
                // //x0002 or //X0002
                hex_str=substr(naddr, 7);
            }
            else
            {
                // 0x0002 or 0X0002
                hex_str=substr(naddr, 6);
            }
            print("Got hex ip: [" +hex_str+ "]");
            
            local ex = regexp("^....(..)(..)(..)(..)");
            local res = ex.capture(naddr);
            local ip =  hex2int(hex_str.slice(res[1].begin,res[1].end)) + "." +
                        hex2int(hex_str.slice(res[2].begin,res[2].end)) + "." +
                        hex2int(hex_str.slice(res[3].begin,res[3].end)) + "." +
                        hex2int(hex_str.slice(res[4].begin,res[4].end));
            print("Extract by hex-ip: "+ip);
            M_lmidmachines[idx].ip <- ip;
        }
        else
        {
            
            local ip_start = NET_DEFAULT_IP_START + M_lmidmachines[idx].nodeid;
            local ip = "192.168.88."+ip_start;
            print("Use default addresses: "+ip);
            M_lmidmachines[idx].ip <- ip;
        }
	
    }

}


/**
 * Function parses envfile and performs few transformations
 */
function prep_envfile(instance)
{

    //Key is env name, value is value
    instance.envfile <-{};

    local envfile = get_val(instance.machine, "ENVFILE", [""])[0];
    if (envfile!="")
    {
        print("Parsing env file ["+envfile+"]");
        local myfile = file(envfile,"r");
        
        //Read line by line, process it 
        while (1)
        {

            local line = "";
            try
            {
                line = myfile.readline(1024*10);
            }
            catch (e)
            {
                myfile.close();
                print(e);
                throw e;
            }

            if (line=="" && myfile.eos())
            {
                break;
            }

            if (line=="")
            {
                continue;
            }

            if (regexp(@"^#.*|[ \t\r]+#.*").match(line))
            {
                continue;
            }

            //Process the line
            local ex = regexp("^(.*)=(.*)");
            local res = ex.capture(line);

            if (res.len()!=3)
            {
                error("Invalid envfile line ["+line+"] - ignoring");
                continue;
            }

            local envname = line.slice(res[1].begin,res[1].end);
            local envvalue = line.slice(res[2].begin,res[2].end);

            if (envname=="FIELDTBLS32")
            {
                envname="FIELDTBLS";
            }

            if (envname=="FLDTBLDIR32")
            {
                envname="FLDTBLDIR";
            }

            if (envname=="VIEWDIR32")
            {
                envname="VIEWDIR";
            }

            if (envname=="VIEWFILES32")
            {
                envname="VIEWFILES";
            }

            //Additional processing
            if (envname=="FIELDTBLS")
            {
                envvalue=envvalue+",Exfields";
            }

            if (envname=="FLDTBLDIR")
            {
                envvalue=envvalue+":${NDRX_HOME}/share/endurox/ubftab";
            }

            print(format("Adding env from envfile [%s] = [%s]", envname, envvalue));
            instance.envfile[envname]<-envvalue;
        }

        myfile.close();
    }

}

/**
 * Generate set file
 * @param instance current object of interest
 */
function gen_set_file(instance)
{
    //Prepare set file
    //Needs to set CCONFIG to the file, not the folder, if several configs
    //live there, it will load them all.
    instance.set_text <-
/******************************************************************************/
@"#/bin/bash
#
# @(#) Load this script in environment before Enduro/X start
#

# update to correspond actual Enduro/X installation path
export NDRX_HOME=/usr
export NDRX_APPHOME="+instance.app_home+@"
export NDRX_CCONFIG="+instance.ndrx_conf+"/"+"app."+instance.prefix+@".ini
export CDPATH=$CDPATH:.:${NDRX_APPHOME}
export PATH=$PATH:"+instance.ndrx_bin;
/******************************************************************************/

    //Add additional folders to the path, if exists ...

    foreach (idx, folder in instance.appdir_add)
    {
        if (folder.use_rel)
        {
            instance.set_text+=":${NDRX_APPHOME}/"+folder.path;
        }
        else
        {
            instance.set_text+=":"+folder.path;
        }
    }
    instance.set_text+="\n";

    print("set_file: [\n"+instance.set_text+"]");
}

/**
 * Generate ini file.
 * This contains sections:
 * - global env
 * - cctag/group envs
 * - debug settings
 * - forward queues, if any
 */
function gen_ini_file(instance)
{
    instance.ini_text <-
/******************************************************************************/
@"[@global]
NDRX_CLUSTERISED=1
NDRX_CMDWAIT=1
NDRX_CONFIG="+instance.ndrx_conf+"/ndrxconfig."+instance.prefix+@".xml
NDRX_ULOG="+instance.log_rel+@"
NDRX_DMNLOG=${NDRX_ULOG}/ndrxd.log
NDRX_DPID=${NDRX_APPHOME}/tmp/ndrxd."+instance.prefix+@".pid
NDRX_DQMAX=100
NDRX_IPCKEY="+instance.ipckey+@"
NDRX_LDBAL=0
NDRX_LEV=5
NDRX_LOG=${NDRX_ULOG}/xadmin.log
NDRX_MSGMAX=100
NDRX_MSGSIZEMAX=56000
NDRX_NODEID="+instance.machine.nodeid+@"
NDRX_QPATH="+M_wizzard.qpath+@"
NDRX_QPREFIX=/"+instance.prefix+@"
NDRX_RNDK="+rands(8)+@"
NDRX_SRVMAX="+ M_resources["MAXSERVERS"][0]+@"
NDRX_SVCMAX="+M_resources["MAXSERVICES"][0]+@"
NDRX_TOUT="+ M_resources["SCANUNIT"][0].tointeger() 
      * M_resources["BLOCKTIME"][0].tointeger() +@"
NDRX_UBFMAXFLDS=16000
NDRX_LIBEXT="+M_wizzard.shared_lib_pfx+@"
# TODO, replace if not found already in env:
# If present in envfile, these will be imported bellow.
#FIELDTBLS=Exfields
#FLDTBLDIR=${NDRX_APPHOME}/ubftab
NDRX_RTSVCMAX="+instance.rtsvcmax+@"
NDRX_RTCRTMAX="+M_resources["MAXRTDATA"][0]+@"
";
   foreach(env, val in instance.envfile)
   {
        instance.ini_text+=format("%s=%s\n", env, val);
   }
/******************************************************************************/

    /* per group / cctag settings: */
    foreach(idx,igroup in M_sections["*GROUPS"].params)
    {
        local group = igroup[0];
        //Get our groups
        if (!group.is_default &&
            get_val(group, "LMID", null)[0] == instance.lmid && group.plot)
        {
            local group_text ="[@global/"+group.name+"]";

            //Support for null switches...
            if ("tmsname" in group && "driverlib" in group)
            {
                group_text+=
@"
NDRX_XA_RES_ID="+group.rmid+@"
NDRX_XA_OPEN_STR="+group.openinfo+@"
NDRX_XA_CLOSE_STR="+group.closeinfo+@"
NDRX_XA_DRIVERLIB="+group.driverlib+@"
NDRX_XA_RMLIB="+group.rmlib+@"
NDRX_XA_LAZY_INIT=0
NDRX_XA_FLAGS=FDATASYNC;DSYNC";
/******************************************************************************/
            }
            else if ("xaswitchname" in group && "TUXEDO/QM"==group.xaswitchname)
            {
                group_text+=
@"
NDRX_XA_RES_ID="+group.rmid+@"
NDRX_XA_OPEN_STR="+group.openinfo+@"
NDRX_XA_CLOSE_STR="+group.closeinfo+@"
NDRX_XA_DRIVERLIB=libndrxxaqdisks."+M_wizzard.shared_lib_pfx+@"
NDRX_XA_RMLIB=libndrxxaqdisk."+M_wizzard.shared_lib_pfx+@"
NDRX_XA_LAZY_INIT=0
NDRX_XA_FLAGS=FDATASYNC;DSYNC";
/******************************************************************************/
            }
            else if ("xaswitchname" in group)
            {
                group_text+=
/******************************************************************************/
@"
NDRX_XA_RES_ID="+group.rmid+@"
NDRX_XA_OPEN_STR="+group.openinfo+@"
NDRX_XA_CLOSE_STR="+group.closeinfo+@"
# use built in switch resolver
NDRX_XA_DRIVERLIB=libndrxxatmsx."+M_wizzard.shared_lib_pfx+@"
NDRX_XA_RMLIB=-
NDRX_XA_LAZY_INIT=1
NDRX_XA_FLAGS=RECON:*:3:100;FDATASYNC;DSYNC";
/******************************************************************************/
            }

            if ("routed" in group)
            {
                group_text+="\nNDRX_RTGRP="+group.name;
            }
            
            instance.ini_text+="\n"+group_text+"\n";
            
        }
    }

    // Debugs per binary:
    // Also needs to decide is it relative path or full path...
    // And if it contains  sub-folders (as with appdir), recursively we shall
    // create them.
    instance.ini_text+="\n"+
/******************************************************************************/
@"[@debug]
#* - goes for all binaries not listed bellow
*= ndrx=3 ubf=1 tp=3 threaded=l file=${NDRX_ULOG}/endurox.org
xadmin=file=${NDRX_ULOG}/xadmin.log
ndrxd=file=${NDRX_ULOG}/ndrxd.log
";
/******************************************************************************/
    
    //Plot server debug only once.
    local server_plotted={};
    foreach (idx, server in instance.servers) if (!server.is_default)
    {
        if (!(server.bin in server_plotted))
        {
            //Add debug entry.
            instance.ini_text+=format("%s=file=${NDRX_ULOG}/%s.${NDRX_SVSRVID}.log\n", 
                server.bin, server.bin);
            server_plotted[server.bin] <-true;
        }
    }

    if (instance.restin)
    {
        instance.ini_text+=format("restincl=file=${NDRX_ULOG}/restincl.rin1.log\n");
    }
    
    //Process the queues now
    foreach(idx,igroup in M_sections["*GROUPS"].params)
    {
        //Single group only supported...
        local group = igroup[0];
        //Get our groups
        if ( !group.is_default && (get_val(group, "LMID", null)[0] == instance.lmid) 
                && "qspace" in group)
        {
            instance.ini_text+="\n"+
@"[@queue/"+group.name+@"]
# Review as necessary, see q.conf man page for details
@=svcnm=@,autoq=n,tries=3,waitinit=0,waitretry=30,waitretrymax=90,memonly=n,mode=fifo,workers=1
";
            //Add forward queues
            foreach (indx,qq in group.auto_queues)
            {
                instance.ini_text+=format("%s=autoq=y", indx, indx);
                if (qq.workers!="1")
                {
                    instance.ini_text+=format(",workers=%s", qq.workers);
                }
                
                if (qq.trantime!="-1")
                {
                    instance.ini_text+=format(",txtout=%s", qq.trantime);
                }
                instance.ini_text+="\n";
            }
        }
    }

    //Add restin config, if WSL/JOLT seen
    if (instance.restin)
    {
        instance.ini_text+="\n"+
@"[@restin]
defaults={""errors"":""json2ubf"", ""conv"":""json2ubf""}

# Instance 1, see restincl manpage for the web service formats
[@restin/RIN1]
port=8080
ip=0.0.0.0
# invoke by: http://this.host:8080/SOME_SERVICE1
/SOME_SERVICE1={""svc"":""SOME_SERVICE1""}
/SOME_SERVICE2={""svc"":""SOME_SERVICE2""}
";
    }
    
    print("app.ini: [\n"+instance.ini_text+"]");

}

/**
 * Generate Enduro/X final XML configuration
 */
function gen_xml_file(instance)
{
    /* global settings */
    instance.xml_text <-
/******************************************************************************/
@"<?xml version=""1.0"" ?>
<endurox>
    <appconfig>
        <sanity>1</sanity>
        <brrefresh>5</brrefresh>
        <restart_min>1</restart_min>
        <restart_step>1</restart_step>
        <restart_max>5</restart_max>
        <restart_to_check>20</restart_to_check>
        <gather_pq_stats>Y</gather_pq_stats>
    </appconfig>
";
/******************************************************************************/

    //Process binary by binary...
    local servers_open = false;
    local tab="";
    foreach (idx, server in instance.servers)
    {
        if (server.is_default)
        {

            if (servers_open)
            {
                instance.xml_text+=
"    </servers>\n";
                tab="";
                servers_open=false;
            }
            instance.xml_text+=
"    <defaults>\n";
        }
        else
        {
            if (!servers_open)
            {
                instance.xml_text+=
"    <servers>\n";
                servers_open=true;
                tab="    ";
            }
            instance.xml_text+=
tab+"    <server name=\""+server.bin+"\">\n";
        }

        if ("autokill" in server)
        {
            instance.xml_text+=
tab+"        <autokill>"+server.autokill+"</autokill>\n";
        }
        
        if ("start_max" in server)
        {
            instance.xml_text+=
tab+"        <start_max>"+server.start_max+"</start_max>\n";
        }
        
        if ("pingtime" in server)
        {
            instance.xml_text+=
tab+"        <pingtime>"+server.pingtime+"</pingtime>\n";
        }
        
        if ("ping_max" in server)
        {
            instance.xml_text+=
tab+"        <ping_max>"+server.ping_max+"</ping_max>\n";
        }
        
        if ("end_max" in server)
        {
            instance.xml_text+=
tab+"        <end_max>"+server.end_max+"</end_max>\n";
        }
        
        if ("killtime" in server)
        {
            instance.xml_text+=
tab+"        <killtime>"+server.killtime+"</killtime>\n";
        }
        
        if ("cctag" in server)
        {
            if (server.cctag=="/")
            {
                instance.xml_text+=
tab+"        <cctag/>\n";
            }
            else
            {
                instance.xml_text+=
tab+"        <cctag>"+server.cctag+"</cctag>\n";
            }
        }
        
        if ("min" in server)
        {
            instance.xml_text+=
tab+"        <min>"+server.min+"</min>\n";
        }
        
        if ("max" in server)
        {
            instance.xml_text+=
tab+"        <max>"+server.max+"</max>\n";
        }
        
        if ("srvid" in server)
        {
            instance.xml_text+=
tab+"        <srvid>"+server.srvid+"</srvid>\n";
        }
        
        if ("env" in server)
        {
            instance.xml_text+=
tab+"        <env>"+server.env+"</env>\n";
        }
        
        if ("rqaddr" in server)
        {
            instance.xml_text+=
tab+"        <rqaddr>"+server.rqaddr+"</rqaddr>\n";
        }
        
        if ("respawn" in server)
        {
            instance.xml_text+=
tab+"        <respawn>"+server.respawn+"</respawn>\n";
        }
        
        if ("mindispatchthreads" in server)
        {
            instance.xml_text+=
tab+"        <mindispatchthreads>"+server.mindispatchthreads+"</mindispatchthreads>\n";
        }
        
        if ("maxdispatchthreads" in server)
        {
            instance.xml_text+=
tab+"        <maxdispatchthreads>"+server.maxdispatchthreads+"</maxdispatchthreads>\n";
        }
        
        if ("sysopt" in server)
        {
            instance.xml_text+=
tab+"        <sysopt>"+server.sysopt+"</sysopt>\n";
        }
        
        if ("appopt" in server && server.appopt!="")
        {
            instance.xml_text+=
tab+"        <appopt>"+server.appopt+"</appopt>\n";
        }
        
        if (!server.is_default && "threadstacksize" in server)
        {
            instance.xml_text+=
tab+@"        <envs><env name=""NDRX_THREADSTACKSIZE"">"+server.threadstacksize+"</env></envs>\n";
        }
        
        if (server.is_default)
        {
            instance.xml_text+=
tab+"    </defaults>\n";
        }
        else
        {
            instance.xml_text+=
tab+"    </server>\n";
        }
        
    }

    //Terminate the servers if was open..
    if (servers_open)
    {
        instance.xml_text+=
"    </servers>\n";
        tab="";
        servers_open=false;
    }

    //Write client (restin) if needed.
    if (instance.restin)
    {
        instance.xml_text+=
@"    <clients>
          <client cmdline=""restincl"">
              <exec tag=""RESTIN"" autostart=""Y"" subsect=""RIN1"" cctag=""RIN1"" log=""${NDRX_ULOG}/restincl.rin1.log""/>
          </client>
    </clients>
";
    }

    //Prepare services.
    if (instance.rtservices > 0)
    {
        //Only once
        local service_once={};
        /* global settings */
        instance.xml_text+="    <services>\n";

        //OK, plot services section.
        foreach(idx,service in M_sections["*SERVICES"].order)
        {
            local svcok = false;

            if (!service.is_default)
            {
                if (service_once)
                local range = {};
                local service_srvgrp = get_val(service, "SRVGRP", "");
                
                //Service may have no group -> affect all
                if (service_srvgrp =="")
                {
                    svcok=true;
                }
                else
                {
                    local group_lmid = get_val(
                        M_sections["*GROUPS"].params[service_srvgrp[0]][0], "LMID", null)[0];

                    if (group_lmid == instance.lmid)
                    {
                        svcok=true;
                    }
                }
            }
            else
            {
                svcok=true;
            }

            if (svcok)
            {
                if (service.name in service_once)
                {
                    //Skip this, already exported
                    continue;
                }
                
                service_once[service.name]<-true;

                if (service.is_default)
                {
                    instance.xml_text+="        <defaults";
                }
                else
                {
                    instance.xml_text+="        <service svcnm=\""+service.name+"\"";
                }
    
                if ("PRIO" in service.keywords)
                {
                    instance.xml_text+=" prio=\""+service.keywords["PRIO"][0]+"\"";
                }

                if ("ROUTING" in service.keywords)
                {
                    instance.xml_text+=" routing=\""+service.keywords["ROUTING"][0]+"\"";
                }

                if ("AUTOTRAN" in service.keywords)
                {
                    instance.xml_text+=" autotran=\""+service.keywords["AUTOTRAN"][0]+"\"";
                }

                if ("TRANTIME" in service.keywords)
                {
                    instance.xml_text+=" trantime=\""+service.keywords["TRANTIME"][0]+"\"";
                }

                //close the tag
                instance.xml_text+="/>\n";
            }
        }

        instance.xml_text+="    </services>\n";

    }

    //Prepare routing

    if ("*ROUTING" in M_sections)
    {
        local rt_started=false;

        foreach(idx,route in M_sections["*ROUTING"].params)
        {
            //Single routing supported only (afaik Tuxedo also has requires single route)
            route = route[0];
            local field = get_val(route, "FIELD", null)[0];
            local fieldtype = get_val(route, "FIELDTYPE", ["STRING"])[0];
            local ranges = get_val(route, "RANGES", null)[0];
            local buftype = get_val(route, "BUFTYPE", null)[0];

            if (buftype!="FML32" && buftype!="FML")
            {
                error(format("Ignoring route [%s] as buffer type [%s] not supported", 
                        route.name, buftype));
                continue;
            }

            if (!rt_started)
            {
                instance.xml_text+="    <routing>\n";
                rt_started=true;
            }
            
            instance.xml_text+=
@"        <route routing="""+route.name+@""">
            <field>"+field+@"</field>
            <ranges>"+ranges+@"</ranges>
            <buftype>UBF</buftype>
        </route>
";

        }

        if (rt_started)
        {
            instance.xml_text+="    </routing>\n";
        }

    }
    
    instance.xml_text+="</endurox>\n";

    print("ndrxconfig.ini: [\n"+instance.xml_text+"]");
    
}

/**
 * Write configuration files to the disk
 */
function write_files()
{
    local config_exists = false;
    //Verify files (ask for overwrite, if any missing)
    foreach (idx, instance in M_instances)
    {
        if (fileexists(instance.set_file))
        {
            //print file name
            print_stdout("Configuration file exists: ["+instance.set_file+"]\n");
            config_exists = true;
        }

        if (fileexists(instance.ini_file))
        {
            //print file name
            print_stdout("Configuration file exists: ["+instance.ini_file+"]\n");
            config_exists = true;
        }

        if (fileexists(instance.xml_file))
        {
            //print file name
            print_stdout("Configuration file exists: ["+instance.xml_file+"]\n");
            config_exists = true;
        }
    }

    //Ask for confirmation, if needed
    if (config_exists 
            && M_opt_y != "1" 
            && 0==chk_confirm("Really overwrite Enduro/X configuration files?"))
    {
        print("Aborted configuration file generation");
        return;
    }

    //TODO: Create folders (if any missing, log created)
    //Firstly transfer the hashmap to array, sort the array, so that shortest
    //folders come first as we are going to create them (if not exists)

    local dirs = [];

    foreach (idx, dir in M_folder_gen)
    {
        dir.name<-idx;
        dirs.append(dir);
    }

    //sort
    dirs.sort(@(a,b) a.name.len() <=> b.name.len());

    //Check dirs...
    foreach (idx, dir in dirs)
    {
        print("Creating directory ["+dir.name+"]...");
        mkdir(dir.name);
        dir.created<-true;
    }

    //Write files
    foreach (idx, instance in M_instances)
    {
        try
        {
            local out = file(instance.set_file,"w");
            out.writes(instance.set_text);
            out.close();
        } 
        catch (e)
        {
            print(e);
            throw(format("Cannot write [%s] file", instance.set_file));
        }

        M_files_gen.append(instance.set_file);

        //Give chmod to execute the file 
        chmod(instance.set_file, "755");

        try
        {
            local out = file(instance.ini_file,"w");
            out.writes(instance.ini_text);
            out.close();
        }
        catch (e)
        {
            print(e);
            throw(format("Cannot write [%s] file", instance.ini_file));
        }

        M_files_gen.append(instance.ini_file);

        try
        {
            local out = file(instance.xml_file,"w");
            out.writes(instance.xml_text);
            out.close();
        }
        catch (e)
        {
            print(e);
            throw(format("Cannot write [%s] file", instance.xml_file));
        }

        M_files_gen.append(instance.xml_file);
    }

}
/**
 * Generate Enduro/X configs. Callback from ubb2ex. At this point
 * all configs are parsed.
 */
function ex_generate(arg)
{
    local nodeid=1;

    print(format("M_opt_n=[%s]", M_opt_n));
    print(format("M_opt_y=[%s]", M_opt_y));
    print(format("M_opt_L=[%s]", M_opt_L));
    print(format("M_opt_A=[%s]", M_opt_A));
    
    if ("./"==substr(M_opt_P, 0, 2))
    {
        M_opt_P = getcwd() + "/" +substr(M_opt_P, 2);
    }
    else if (M_opt_P!="" && "/"!=substr(M_opt_P, 0, 1))
    {
        //This is relative too
        M_opt_P = getcwd() + "/" + M_opt_P;
    }
    
    print(format("M_opt_P=[%s]", M_opt_P));

    init();

    //Open output objects for each of the machine
    if (!("*MACHINES" in M_sections))
    {
        print("No machines defined");
        return;
    }
    
    foreach(idx,val in M_sections["*MACHINES"].params) if (!val[0].is_default)
    {
        val[0].nodeid<-nodeid;
        val[0].networked<-false;
        nodeid++;
        
        //Map LMID:MACHINE
        local lmid = get_val(val[0], "LMID", null)[0];
        M_lmidmachines[lmid]<-val[0];
    }

    //Assign networking identifiers
    prep_networking();

    //Generate each node
    foreach(idx,val in M_sections["*MACHINES"].params)
    {
        local machine = val[0];

        if (machine.is_default)
        {
            continue;
        }

        local instance = {};
        instance.name <- machine.name;
        instance.lmid <- machine.keywords.LMID[0];

        if (M_opt_L!="" && instance.lmid!=M_opt_L)
        {
            print("Skipping node ["+instance.lmid+"] as -l present");
            continue;
        }

        M_instances[machine.name]<-instance;

        instance.restin <-false;
        instance.xa_used<-false;
        print("Machine: "+instance.name);

        local ubb_appdir = get_val(val[0], "APPDIR", null)[0];
        local ubb_tuxconfig = get_val(val[0], "TUXCONFIG", null)[0];

        if (M_opt_P!="" && M_opt_P!="/")
        {
            ubb_tuxconfig = M_opt_P + ubb_tuxconfig;
        }

        local ubb_appdir_add = [];

        if (null!=ubb_appdir.find(":"))
        {
            //Split the ubb_appdir and use first part for ubb_appdir purposes.
            //the other parts later would be checked for relative/full type
            //and would be filled to ubb_appdir_add variable.

            local appdirs =  split(ubb_appdir, ":");
            ubb_appdir = appdirs[0];

            local len = appdirs.len();

            for (local i=1; i<len; i++)
            {
                local add = {};
                
                if (M_opt_P!="" && M_opt_P!="/")
                {
                    add.orgpath<- (M_opt_P + appdirs[i]);
                }
                else
                {
                    add.orgpath<- appdirs[i];
                }

                ubb_appdir_add.append(add);
            }
        }

        if (M_opt_P!="" && M_opt_P!="/")
        {
            ubb_appdir = M_opt_P + ubb_appdir;
        }
        
        instance.prefix <- machine.keywords["LMID"][0].tolower();
        instance.app_home <- strcommon(ubb_appdir, ubb_tuxconfig);
        instance.use_rel <- false;

        if (instance.app_home=="/")
        {
            //this is absolute path
            instance.app_home = ubb_appdir;
            instance.app_bin <- ubb_appdir;
            instance.app_conf <- dirname(ubb_tuxconfig);

            instance.ndrx_conf<- instance.app_conf;
            instance.ndrx_bin <- instance.app_bin;

            instance.set_file <-format("%s/set.%s", instance.app_conf, instance.prefix);
            instance.xml_file <-format("%s/ndrxconfig.%s.xml", instance.app_conf, instance.prefix);
            instance.ini_file <-format("%s/app.%s.ini", instance.app_conf, instance.prefix);

        }
        else
        {
            //get relative path, strip off the last / from the string
            instance.app_home = rstrips(instance.app_home, "/");

            if (instance.app_home=="")
            {
                //Exception case, not nice..
                instance.app_home="/";
            }

            instance.app_bin <- strdiff(ubb_appdir, instance.app_home);
            instance.app_bin = lstrips(instance.app_bin, "/");

            instance.app_conf <- strdiff(dirname(ubb_tuxconfig),
                                            instance.app_home);
            instance.app_conf = lstrips(instance.app_conf, "/");

            instance.use_rel = true;

            //Prepare further folders

            local conf_full = instance.app_home;
            if (instance.app_conf!="")
            {
                instance.ndrx_conf <- "${NDRX_APPHOME}/"+instance.app_conf;
                conf_full+="/"+instance.app_conf;
            }
            else
            {
                instance.ndrx_conf <- "${NDRX_APPHOME}";
            }

            if (instance.app_bin!="")
            {
                instance.ndrx_bin <- "${NDRX_APPHOME}/"+instance.app_bin;
            }
            else
            {
                instance.ndrx_bin <- "${NDRX_APPHOME}";
            }

            instance.set_file <-format("%s/set.%s", conf_full, instance.prefix);
            instance.xml_file <-format("%s/ndrxconfig.%s.xml", conf_full, instance.prefix);
            instance.ini_file <-format("%s/app.%s.ini", conf_full, instance.prefix);
        }

        //Process additional folders
        foreach(dix, folder in ubb_appdir_add)
        {
            local common = strcommon(folder.orgpath, instance.app_home)
                
            //Schedule folder anyway
            //M_folder_gen[folder.orgpath]<-{};
            schedule_directory(folder.orgpath);

            if (common!=instance.app_home)
            {
                //Use full path ..
                folder.path <- folder.orgpath;
                folder.use_rel <- false;
            }
            else
            {
                folder.path <- strdiff(folder.orgpath, instance.app_home);
                folder.path=lstrips(folder.path, "/");
                folder.use_rel <- true;
            }
        }

        //Additional folders to be added
        instance.appdir_add <- ubb_appdir_add;
        instance.nodeid <- machine.nodeid;

        if (regexp("^0x.*|^0X.*").match(M_resources["IPCKEY"][0]))
        {
            instance.ipckey <- substr(M_resources["IPCKEY"][0], 2);
        }
        else
        {
            instance.ipckey <- M_resources["IPCKEY"][0]
        }

        //Resolve logging directory also... full or relative against the home
        local log_dir = get_val(val[0], "ULOGPFX", [""])[0];

        if (log_dir=="")
        {
            instance.log_full <- instance.app_home + "/log";
            instance.log_rel <- "${NDRX_APPHOME}/log";
        }
        else
        {
            //Add prefix
            if (M_opt_P!="" && M_opt_P!="/")
            {
                log_dir = M_opt_P + log_dir;
            }

            //Just take dirname 
            log_dir = dirname(log_dir);            

            if (log_dir==instance.app_home)
            {
                //Use log anyway
                instance.log_full <- instance.app_home + "/log";
                instance.log_rel <- "${NDRX_APPHOME}/log";
            }
            else
            {
                local common = strcommon(log_dir, instance.app_home);
                local diff = strdiff(log_dir, instance.app_home);
                
                diff = lstrips(diff, "/");

                if (common!=instance.app_home)
                {
                    //use full path to logs...
                    instance.log_full <- log_dir;
                    instance.log_rel <- log_dir;
                }
                else 
                {
                    //Relative to app home with log suffix...
                    instance.log_full <- instance.app_home+"/"+diff;
                    instance.log_rel <- "${NDRX_APPHOME}/"+diff;
                }
            }
        }

        //Dump some config
        print("use_rel = "+instance.use_rel);
        print("nodeid = "+instance.nodeid);
        print("prefix = "+instance.prefix);
        print("app_home = "+instance.app_home);
        print("app_conf = "+instance.app_conf);
        print("app_bin = "+instance.app_bin);

        print("set_file = "+instance.set_file);
        print("ini_file = "+instance.ini_file);
        print("xml_file = "+instance.xml_file);

        //Load Tuxedo defaults
        if (!("BLOCKTIME" in M_resources))
        {
            M_resources["BLOCKTIME"]<-[];
            M_resources["BLOCKTIME"].append("6");
        }

        if (!("SCANUNIT" in M_resources))
        {
            M_resources["SCANUNIT"]<-[];
            M_resources["SCANUNIT"].append("10");
        }

        //Use Enduro/X default 20K
        if (!("MAXSERVICES" in M_resources) || M_resources["MAXSERVICES"][0].tointeger() < 20000)
        {
            M_resources["MAXSERVICES"]<-[];
            M_resources["MAXSERVICES"].append("20000");
        }

        if (!("MAXSERVERS" in M_resources) || M_resources["MAXSERVERS"][0].tointeger() < 10000)
        {
            M_resources["MAXSERVERS"]<-[];
            M_resources["MAXSERVERS"].append("10000");
        }

        if (!("MAXSERVERS" in M_resources) || M_resources["MAXSERVERS"][0].tointeger() < 10000)
        {
            M_resources["MAXSERVERS"]<-[];
            M_resources["MAXSERVERS"].append("10000");
        }

        //Default transaction time
        if (!("MAXTRANTIME" in M_resources))
        {
            M_resources["MAXTRANTIME"]<-[];
            M_resources["MAXTRANTIME"].append("2147483647");
        }

        if (!("MAXRTDATA" in M_resources) || M_resources["MAXRTDATA"][0].tointeger() < 102400)
        {
            M_resources["MAXRTDATA"]<-[];
            M_resources["MAXRTDATA"].append("102400");
        }

        //Calculate number of routing services, if any.
        instance.rtservices<-get_routing_services(instance);

        //resources setting
        instance.rtsvcmax<-get_routing_services(instance);

        //Use fault, if not any set.
        if (instance.rtsvcmax == 0)
        {
            instance.rtsvcmax = 1000;
        }
        else
        {
            //For performance reasons use more.
            instance.rtsvcmax *=2;
        }

        //Schedule system folders
        schedule_directory(instance.app_home);
        schedule_directory(instance.log_full);
        schedule_directory(instance.app_home+"/tmp");

        if (instance.use_rel)
        {
            schedule_directory(instance.app_home+"/"+instance.app_bin);
            schedule_directory(instance.app_home+"/"+instance.app_conf);
        }
        else
        {
            //M_folder_gen[instance.app_conf]<-{};
            schedule_directory(instance.app_conf);
        }

        instance.machine<-machine;
        
        //Process groups
        prep_groups(instance);
    
        //Prepare ranges
        prep_free_ranges(instance);
        
        //Build server listings
        prep_servers(instance);

        //Load the instance environment
        prep_envfile(instance);

        //Generate file contents:
        gen_set_file(instance);
        gen_ini_file(instance);
        gen_xml_file(instance);
    }

    if (M_opt_n!="1")
    {
        write_files();
    }
}

/**
 * Cleanup call when script have failed.
 * @param arg not used
 */
function ex_cleanup(arg)
{
    print("File cleanup...");
    //Remove created files
    foreach (idx, file in M_files_gen)
    {
        try
        {
            print("Removing ["+file+"]");
            unlink(file);
        }
        catch (e)
        {
            error(e);
        }
    }

    print("Folder cleanup...");
    //Remove created directories...
    local dirs = [];

    foreach (idx, dir in M_folder_gen)
    {
        dir.name<-idx;
        dirs.append(dir);
    }

    //sort, reverse order, deepest dir first...
    dirs.sort(@(a,b) b.name.len() <=>a.name.len());

    //Check dirs...
    foreach (idx, dir in dirs)
    {
        if ("created" in dir)
        {
            print("Removing directory ["+dir.name+"]...");
            try
            {
                rmdir(dir.name);
            }
            catch (e)
            {
                //Just print error..
                error(e);
            }
        }
    }
}
